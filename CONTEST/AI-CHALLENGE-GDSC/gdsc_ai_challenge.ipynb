{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "gdsc-ai-challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import os, random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import (Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, \n",
        "                                    GlobalAveragePooling2D, UpSampling2D, Input, AlphaDropout)\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
        "#from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
        "from PIL import Image\n",
        "\n",
        "#gcs = KaggleDatasets().get_gcs_path(\"tfrecordgdsc\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:30:06.453435Z",
          "iopub.execute_input": "2022-01-08T10:30:06.453859Z",
          "iopub.status.idle": "2022-01-08T10:30:11.381252Z",
          "shell.execute_reply.started": "2022-01-08T10:30:06.453746Z",
          "shell.execute_reply": "2022-01-08T10:30:11.380231Z"
        },
        "trusted": true,
        "id": "ts9cNZh7pJYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"../input/gdsc-ai-challenge/sampleSubmission.csv\")\n",
        "train = pd.read_csv(\"../input/gdsc-ai-challenge/trainLabels.csv\")\n",
        "\n",
        "train[\"id\"] = train[\"id\"].astype(str)\n",
        "test[\"id\"] = test[\"id\"].astype(str)\n",
        "\n",
        "test = test.drop(columns = \"label\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:30:11.382939Z",
          "iopub.execute_input": "2022-01-08T10:30:11.383224Z",
          "iopub.status.idle": "2022-01-08T10:30:11.556911Z",
          "shell.execute_reply.started": "2022-01-08T10:30:11.383192Z",
          "shell.execute_reply": "2022-01-08T10:30:11.55617Z"
        },
        "trusted": true,
        "id": "glt7n2ybpJYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"../input/gdsc-ai-challenge/train/train/\"\n",
        "test_path = \"../input/gdsc-ai-challenge/test/test/\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:30:11.558147Z",
          "iopub.execute_input": "2022-01-08T10:30:11.558402Z",
          "iopub.status.idle": "2022-01-08T10:30:11.562853Z",
          "shell.execute_reply.started": "2022-01-08T10:30:11.558369Z",
          "shell.execute_reply": "2022-01-08T10:30:11.561901Z"
        },
        "trusted": true,
        "id": "nf0TutE2pJYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"id\"] = [train_path] + train[\"id\"].values + [\".png\"]\n",
        "test[\"id\"] = [test_path] + test[\"id\"].values + [\".png\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:30:11.56417Z",
          "iopub.execute_input": "2022-01-08T10:30:11.564753Z",
          "iopub.status.idle": "2022-01-08T10:30:11.615568Z",
          "shell.execute_reply.started": "2022-01-08T10:30:11.5647Z",
          "shell.execute_reply": "2022-01-08T10:30:11.614864Z"
        },
        "trusted": true,
        "id": "LvCLFi7RpJYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(set(train[\"label\"].values))\n",
        "encoded_label = pd.get_dummies(labels).values\n",
        "map_label = dict(zip(labels, encoded_label))\n",
        "train[\"encoded_label\"] = train[\"label\"].map(map_label)\n",
        "\n",
        "# Reading the data\n",
        "training_images = np.array([plt.imread(f\"{train_path}{i + 1}.png\") for i in range(0, 50000)])\n",
        "testing_images  = np.array([plt.imread(f\"{test_path}{i + 1}.png\") for i in range(0, 20000)])\n",
        "    \n",
        "training_images = training_images.astype(\"float32\")\n",
        "testing_images = testing_images.astype(\"float32\")   \n",
        "    \n",
        "#training_images /= 255.0\n",
        "#testing_images /= 255.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:30:11.617772Z",
          "iopub.execute_input": "2022-01-08T10:30:11.618343Z",
          "iopub.status.idle": "2022-01-08T10:34:08.101668Z",
          "shell.execute_reply.started": "2022-01-08T10:30:11.618291Z",
          "shell.execute_reply": "2022-01-08T10:34:08.100607Z"
        },
        "trusted": true,
        "id": "JIwo9OT3pJYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training_images = tf.keras.applications.inception_resnet_v2.preprocess_input(training_images)\n",
        "#testing_images = tf.keras.applications.inception_resnet_v2.preprocess_input(testing_images)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:08.103322Z",
          "iopub.execute_input": "2022-01-08T10:34:08.103591Z",
          "iopub.status.idle": "2022-01-08T10:34:08.108024Z",
          "shell.execute_reply.started": "2022-01-08T10:34:08.103559Z",
          "shell.execute_reply": "2022-01-08T10:34:08.107009Z"
        },
        "trusted": true,
        "id": "q82NaOFSpJYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mappings, labeling_images = np.unique(train[\"label\"].to_numpy(copy = True), return_inverse = True)\n",
        "training_labels = labeling_images.reshape(-1, 1)\n",
        "\n",
        "labeling_images = tf.keras.utils.to_categorical(training_labels, 10)\n",
        "labeling_images"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:08.109608Z",
          "iopub.execute_input": "2022-01-08T10:34:08.110087Z",
          "iopub.status.idle": "2022-01-08T10:34:08.179187Z",
          "shell.execute_reply.started": "2022-01-08T10:34:08.11004Z",
          "shell.execute_reply": "2022-01-08T10:34:08.178454Z"
        },
        "trusted": true,
        "id": "uM0Mu6dspJYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ShearX(img, v):\n",
        "    assert -0.3 <= v <= 0.3\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
        "\n",
        "def ShearY(img, v):\n",
        "    assert -0.3 <= v <= 0.3\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
        "\n",
        "def TranslateX(img, v):\n",
        "    assert -0.45 <= v <= 0.45\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    v = v * img.size[0]\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
        "\n",
        "def TranslateXabs(img, v):\n",
        "    assert 0 <= v\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
        "\n",
        "def TranslateY(img, v):\n",
        "    assert -0.45 <= v <= 0.45\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    v = v * img.size[1]\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
        "\n",
        "def TranslateYabs(img, v):\n",
        "    assert 0 <= v\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
        "\n",
        "def Rotate(img, v):\n",
        "    assert -30 <= v <= 30\n",
        "    if random.random() > 0.5:\n",
        "        v = -v\n",
        "    return img.rotate(v)\n",
        "\n",
        "def AutoContrast(img, _):\n",
        "    return PIL.ImageOps.autocontrast(img)\n",
        "\n",
        "def Invert(img, _):\n",
        "    return PIL.ImageOps.invert(img)\n",
        "\n",
        "def Equalize(img, _):\n",
        "    return PIL.ImageOps.equalize(img)\n",
        "\n",
        "def Mirror(img, _):\n",
        "    return PIL.ImageOps.mirror(img)\n",
        "\n",
        "def Solarize(img, v):\n",
        "    assert 0 <= v <= 256\n",
        "    return PIL.ImageOps.solarize(img, v)\n",
        "\n",
        "def SolarizeAdd(img, addition=0, threshold=128):\n",
        "    img_np = np.array(img).astype(np.int)\n",
        "    img_np = img_np + addition\n",
        "    img_np = np.clip(img_np, 0, 255)\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    img = Image.fromarray(img_np)\n",
        "    return PIL.ImageOps.solarize(img, threshold)\n",
        "\n",
        "def Posterize(img, v):\n",
        "    v = int(v)\n",
        "    v = max(1, v)\n",
        "    return PIL.ImageOps.posterize(img, v)\n",
        "\n",
        "def Contrast(img, v):\n",
        "    assert 0.1 <= v <= 1.9\n",
        "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
        "\n",
        "def Color(img, v):\n",
        "    assert 0.1 <= v <= 1.9\n",
        "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
        "\n",
        "def Brightness(img, v):\n",
        "    assert 0.1 <= v <= 1.9\n",
        "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
        "\n",
        "def Sharpness(img, v):\n",
        "    assert 0.1 <= v <= 1.9\n",
        "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
        "\n",
        "def Cutout(img, v):\n",
        "    assert 0.0 <= v <= 0.2\n",
        "    if v <= 0.:\n",
        "        return img\n",
        "\n",
        "    v = v * img.size[0]\n",
        "    return CutoutAbs(img, v)\n",
        "\n",
        "def CutoutAbs(img, v):\n",
        "    if v < 0:\n",
        "        return img\n",
        "    w, h = img.size\n",
        "    x0 = np.random.uniform(w)\n",
        "    y0 = np.random.uniform(h)\n",
        "\n",
        "    x0 = int(max(0, x0 - v / 2.))\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = min(w, x0 + v)\n",
        "    y1 = min(h, y0 + v)\n",
        "\n",
        "    xy = (x0, y0, x1, y1)\n",
        "    color = (125, 123, 114)\n",
        "    # color = (0, 0, 0)\n",
        "    img = img.copy()\n",
        "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
        "    return img\n",
        "\n",
        "def SamplePairing(imgs):\n",
        "    def f(img1, v):\n",
        "        i = np.random.choice(len(imgs))\n",
        "        img2 = PIL.Image.fromarray(imgs[i])\n",
        "        return PIL.Image.blend(img1, img2, v)\n",
        "    return f\n",
        "\n",
        "def Identity(img, v):\n",
        "    return img\n",
        "\n",
        "def augment_list():\n",
        "    l = [\n",
        "        (AutoContrast, 0, 1),\n",
        "        (Equalize, 0, 1),\n",
        "        (Invert, 0, 1),\n",
        "        (Rotate, 0, 30),\n",
        "        (Posterize, 0, 4),\n",
        "        (Solarize, 0, 256),\n",
        "        (SolarizeAdd, 0, 110),\n",
        "        (Color, 0.1, 1.9),\n",
        "        (Contrast, 0.1, 1.9),\n",
        "        (Brightness, 0.1, 1.9),\n",
        "        (Sharpness, 0.1, 1.9),\n",
        "        (ShearX, 0., 0.3),\n",
        "        (ShearY, 0., 0.3),\n",
        "        (CutoutAbs, 0, 40),\n",
        "        (TranslateXabs, 0., 100),\n",
        "        (TranslateYabs, 0., 100),\n",
        "        (Mirror, 0., 0.)\n",
        "    ]\n",
        "    return l\n",
        "\n",
        "class RandAugment:\n",
        "    def __init__(self, n, m):\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_list = augment_list()\n",
        "\n",
        "    def __call__(self, img): # img shape: [batch_size, 32, 32, 3]\n",
        "        for i in range(img.shape[0]):\n",
        "            img_temp = Image.fromarray(np.uint8(img[i])).convert(\"RGB\")\n",
        "            ops = random.choices(self.augment_list, k = self.n)\n",
        "            for op, minval, maxval in ops:\n",
        "                val = (float(self.m) / 30) * float(maxval - minval) + minval\n",
        "                img[i] = np.array(op(img_temp, val), dtype = \"float32\")\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.244444Z",
          "iopub.execute_input": "2022-01-08T10:34:09.244719Z",
          "iopub.status.idle": "2022-01-08T10:34:09.28202Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.244686Z",
          "shell.execute_reply": "2022-01-08T10:34:09.281123Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "CNPUZpqkpJYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN():\n",
        "    cnn = Sequential()\n",
        "    \n",
        "    cnn.add(Conv2D(filters = 32, kernel_size = 3, padding = \"same\", activation = \"relu\", input_shape = (32, 32, 3)))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Conv2D(filters = 32, kernel_size = 3, padding = \"same\", activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = 2))\n",
        "    cnn.add(Dropout(0.3))\n",
        "\n",
        "    cnn.add(Conv2D(filters = 64, kernel_size = 3, padding = \"same\", activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Conv2D(filters = 64, kernel_size = 3, padding = \"same\", activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = 2))\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(filters = 128, kernel_size = 3, padding = \"same\", activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Conv2D(filters = 128, kernel_size = 3, padding = \"same\", activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = 2))\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Flatten())\n",
        "    cnn.add(Dense(128, activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "    cnn.add(Dense(len(labels), activation = \"softmax\"))\n",
        "    return cnn\n",
        "\n",
        "def CNN2():\n",
        "    weight_decay = 0.0005\n",
        "    cnn  =  Sequential()\n",
        "\n",
        "    cnn.add(Conv2D(64, (3, 3), padding = \"same\", input_shape  =  (32, 32, 3), \n",
        "                   kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.3))\n",
        "\n",
        "    cnn.add(Conv2D(64, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "    cnn.add(Conv2D(128, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(128, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "    cnn.add(Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "\n",
        "    cnn.add(Conv2D(512, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(512, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(512, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "\n",
        "    cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "\n",
        "    cnn.add(Conv2D(512, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(512, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.4))\n",
        "\n",
        "    cnn.add(Conv2D(512, (3, 3), padding = \"same\", kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Flatten())\n",
        "    cnn.add(Dense(512, kernel_regularizer = L2(weight_decay), activation = \"relu\"))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Dropout(0.5))\n",
        "    cnn.add(Dense(len(labels), activation = \"softmax\"))\n",
        "    return cnn\n",
        "\n",
        "def CNN3():\n",
        "    inputs = Input(shape = (32, 32, 3))\n",
        "    \n",
        "    resize = UpSampling2D(size = (7, 7))(inputs)\n",
        "    resnet = tf.keras.applications.resnet.ResNet50(input_shape = (224, 224, 3), include_top = False,\n",
        "                        weights = \"imagenet\")(resize)\n",
        "    resnet.trainable = False\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(resnet)\n",
        "    #x = Flatten()(x)\n",
        "    x = Dense(1024, activation = \"relu\")(x)\n",
        "    x = Dense(512, activation = \"relu\")(x)\n",
        "    x = Dense(len(labels), activation = \"softmax\")(x)\n",
        "    \n",
        "    cnn = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "    return cnn\n",
        "    \n",
        "def CNN4():\n",
        "    inputs = Input(shape = (32, 32, 3))\n",
        "    \n",
        "    resize = UpSampling2D(size = (7, 7))(inputs)\n",
        "    resnet = tf.keras.applications.vgg16.VGG16(input_shape = (224, 224, 3), include_top = False, \n",
        "                            weights = \"imagenet\")(resize)\n",
        "    resnet.trainable = False\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(resnet)\n",
        "    #x = Flatten()(x)\n",
        "    x = Dense(1024, activation = \"relu\")(x)\n",
        "    x = Dense(512, activation = \"relu\")(x)\n",
        "    x = Dense(len(labels), activation = \"softmax\")(x)\n",
        "    \n",
        "    cnn = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "    return cnn\n",
        "\n",
        "def CNN5():\n",
        "    inputs = Input(shape = (32, 32, 3))\n",
        "    \n",
        "    resize = UpSampling2D(size = (7, 7))(inputs)\n",
        "    resnet = tf.keras.applications.resnet.ResNet152(input_shape = (224, 224, 3), include_top = False,\n",
        "                        weights = \"imagenet\")(resize)\n",
        "    resnet.trainable = False\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(resnet)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(1024, activation = \"relu\")(x)\n",
        "    x = Dense(512, activation = \"relu\")(x)\n",
        "    x = Dense(len(labels), activation = \"softmax\")(x)\n",
        "    \n",
        "    cnn = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "    return cnn\n",
        "\n",
        "def CNN6():\n",
        "    with strategy.scope():\n",
        "        inputs = Input(shape = (32, 32, 3))\n",
        "\n",
        "        resize = UpSampling2D(size = (7, 7))(inputs)\n",
        "        eff = tf.keras.applications.efficientnet.EfficientNetB6(input_shape = (224, 224, 3), include_top = False,\n",
        "                            weights = \"imagenet\")(resize)\n",
        "        eff.trainable = False\n",
        "\n",
        "        x = GlobalAveragePooling2D()(eff)\n",
        "        #x = Flatten()(x)\n",
        "        #x = Dense(1024, activation = \"relu\")(x)\n",
        "        x = Dense(512, activation = \"relu\")(x)\n",
        "        x = Dense(len(labels), activation = \"softmax\")(x)\n",
        "\n",
        "        cnn = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "    return cnn\n",
        "\n",
        "def CNN7():\n",
        "    with strategy.scope():\n",
        "        inputs = Input(shape = (32, 32, 3))\n",
        "\n",
        "        #augment = Augmentation(inputs)\n",
        "        \n",
        "        resize = UpSampling2D(size = (7, 7))(inputs)\n",
        "        resnet = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape = (224, 224, 3), \n",
        "                                                                             include_top = False,\n",
        "        weights = \"imagenet\")(resize)\n",
        "        resnet.trainable = False\n",
        "\n",
        "        x = GlobalAveragePooling2D()(resnet)\n",
        "        #x = Flatten()(x)\n",
        "        #x = tf.keras.layers.GaussianNoise(stddev = 0.1)(x)\n",
        "        x = Dense(1024, activation = \"relu\")(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(512, activation = \"relu\")(x)\n",
        "        x = Dense(len(labels), activation = \"softmax\")(x)\n",
        "\n",
        "        cnn = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "    return cnn\n",
        "\n",
        "def CNN8():\n",
        "    # https://tfhub.dev/google/bit/m-r152x4/1\n",
        "    # https://tfhub.dev/google/bit/m-r50x1/1\n",
        "    # https://tfhub.dev/google/bit/m-r101x3/1\n",
        "    module = tf.keras.models.load_model(\"../input/bit-model/bit_m-r50x1_1\")\n",
        "    with strategy.scope():\n",
        "        inputs = Input(shape = (32, 32, 3))\n",
        "\n",
        "        augment = Augmentation(inputs)\n",
        "        \n",
        "        resize = UpSampling2D(size = (5, 5))(augment)\n",
        "        \n",
        "        features = module(resize)\n",
        "        \n",
        "        x = Dense(1024, activation = \"relu\")(features)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(512, activation = \"relu\")(x)\n",
        "        x = Dense(len(labels), activation = \"softmax\")(x)\n",
        "\n",
        "        cnn = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "    return cnn\n",
        "\n",
        "def Augmentation(x):\n",
        "    #aug = RandAugment(n = 2, m = 14)\n",
        "    x = tf.image.flip_left_right(x)\n",
        "    #x = tf.image.flip_up_down(x)\n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    #x = tf.image.random_flip_up_down(x)\n",
        "    #x = tf.keras.layers.GaussianNoise(stddev = 0.05)(x, training = True)\n",
        "    #x = preprocessing.RandomFlip(mode = \"horizontal_and_vertical\")(x, training = True)\n",
        "    #x = preprocessing.RandomRotation(factor = (-0.25, 0.25), fill_mode = \"reflect\")(x, training = True)\n",
        "    x = preprocessing.RandomTranslation(height_factor = (-0.2, 0.2), width_factor = (-0.3, 0.3), \n",
        "                                        fill_mode = \"reflect\")(x, training = True)\n",
        "    #x = preprocessing.RandomZoom(height_factor = (-0.1, 0.1), width_factor = (-0.1, 0.1))(x, training = True)\n",
        "    #x = tf.image.random_saturation(x, -0.2, 0.2)\n",
        "    #x = aug(x)\n",
        "    return x\n",
        "\n",
        "def sample_beta_distribution(size, concentration_0 = 0.2, concentration_1 = 0.2):\n",
        "    gamma_1_sample = tf.random.gamma(shape = [size], alpha = concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape = [size], alpha = concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "def MixUp(images_one, images_two, labels_one, labels_two, alpha = 0.2):\n",
        "    l = sample_beta_distribution(size = 50000, concentration_0 = alpha, concentration_1 = alpha)\n",
        "    x_l = tf.reshape(l, (50000, 1, 1, 1))\n",
        "    y_l = tf.reshape(l, (50000, 1))\n",
        "\n",
        "    images = images_one * x_l + images_two * (1 - x_l)\n",
        "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
        "    return (np.array(images), np.array(labels))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.283711Z",
          "iopub.execute_input": "2022-01-08T10:34:09.28423Z",
          "iopub.status.idle": "2022-01-08T10:34:09.342826Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.284193Z",
          "shell.execute_reply": "2022-01-08T10:34:09.342146Z"
        },
        "trusted": true,
        "id": "c9pfiypnpJYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolynomialDecay():\n",
        "    def __init__(self, maxEpochs = 100, initLR = 0.01, endLR = 0.001, power = 1.0):\n",
        "        self.maxEpochs = maxEpochs\n",
        "        self.initLR = initLR\n",
        "        self.endLR = endLR\n",
        "        self.power = power\n",
        "    def __call__(self, epoch):\n",
        "        decay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
        "        alpha = (self.initLR - self.endLR) * decay + self.endLR\n",
        "        return float(alpha)\n",
        "    \n",
        "class StepDecay():\n",
        "    def __init__(self, initial_LR = 0.01, factor = 0.25, drop_every = 10):\n",
        "        self.initial_LR = initial_LR\n",
        "        self.factor = factor\n",
        "        self.drop_every = drop_every\n",
        "\n",
        "    def __call__(self, epoch):\n",
        "        exp = np.floor((1 + epoch) / self.drop_every)\n",
        "        new_alpha = self.initial_LR * (self.factor ** exp)\n",
        "\n",
        "        return float(new_alpha)\n",
        "\n",
        "def CustomDecay(epoch, lr):\n",
        "      if epoch < 200:\n",
        "        return lr\n",
        "      else: # epoch 200th\n",
        "        factor = (0.001 / 0.00001) / (500 - 200)\n",
        "        return lr * factor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.344181Z",
          "iopub.execute_input": "2022-01-08T10:34:09.344661Z",
          "iopub.status.idle": "2022-01-08T10:34:09.356879Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.34461Z",
          "shell.execute_reply": "2022-01-08T10:34:09.356101Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "u84jT9KPpJY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (32, 32)\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_png(image_data, channels = 3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    return image\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    data = (\n",
        "        {\n",
        "          'image' : tf.io.FixedLenFeature([], tf.string),\n",
        "          'label': tf.io.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "    )\n",
        "    example = tf.io.parse_single_example(example, data)\n",
        "    image = decode_image(example['image'])\n",
        "    label = tf.one_hot(example['label'], 10)\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return image, label\n",
        "\n",
        "def read_unlabeled_tfrecord(example):\n",
        "    data = (\n",
        "        {\n",
        "          'image' : tf.io.FixedLenFeature([], tf.string)\n",
        "        }\n",
        "    )\n",
        "    example = tf.io.parse_single_example(example, data)\n",
        "    image = decode_image(example['image'])\n",
        "    return image\n",
        "\n",
        "def load_dataset(filenames, labeled = True, ordered = False):\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord)\n",
        "    if labeled:\n",
        "        dataset = dataset.map(Preprocess)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def get_training_dataset(filenames, batch_size = 64):\n",
        "    dataset = load_dataset(filenames, labeled = True)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset(filenames, batch_size = 64):\n",
        "    dataset = load_dataset(filenames, labeled = False, ordered = True)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def Preprocess(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    return image, label\n",
        "\n",
        "#training_filenames = tf.io.gfile.glob(f'{gcs}/train*.tfrec')\n",
        "#validation_filenames = tf.io.gfile.glob(f'{gcs}/validation*.tfrec')\n",
        "#test_filenames = tf.io.gfile.glob(f'{gcs}/test*.tfrec')\n",
        "\n",
        "#training_set = get_training_dataset(training_filenames, batch_size = 40)\n",
        "#validation_set = get_training_dataset(validation_filenames, batch_size = 40)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.358428Z",
          "iopub.execute_input": "2022-01-08T10:34:09.35867Z",
          "iopub.status.idle": "2022-01-08T10:34:09.375826Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.358643Z",
          "shell.execute_reply": "2022-01-08T10:34:09.37494Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "tgJ9lYUjpJY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validation_split = 0.1\n",
        "#batch_size = 64\n",
        "#preprocess_vgg = tf.keras.applications.vgg16.preprocess_input\n",
        "#preprocess_resnet = tf.keras.applications.resnet50.preprocess_input\n",
        "#preprocess_effb7 = tf.keras.applications.efficientnet.preprocess_input\n",
        "#preprocess_incep = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
        "\n",
        "#traingen = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.05, height_shift_range = 0.05,\n",
        "#                              horizontal_flip = True, rescale = 1./255, zoom_range = 0.1, shear_range = 10, \n",
        "#                              validation_split = validation_split)\n",
        "#traingen = ImageDataGenerator(rescale = 1./255, validation_split = validation_split, \n",
        "#                              preprocessing_function = preprocess_incep)\n",
        "#traingen.fit(training_images)\n",
        "#training_set_gen = traingen.flow_from_dataframe(dataframe = train, x_col = \"id\", validate_filenames = False, \n",
        "#                                                y_col = \"label\", target_size = (32, 32), batch_size = batch_size,\n",
        "#                                                subset = \"training\", class_mode = \"categorical\") \n",
        "#validation_set_gen = traingen.flow_from_dataframe(dataframe = train, x_col = \"id\", validate_filenames = False, \n",
        "#                                                  y_col = \"label\", target_size = (32, 32), batch_size = batch_size,\n",
        "#                                                  subset = \"validation\", class_mode = \"categorical\") \n",
        "\n",
        "#testgen = ImageDataGenerator(rescale = 1./255, preprocessing_function = preprocess_incep)\n",
        "#test_set_gen = testgen.flow_from_dataframe(dataframe = test, x_col = \"id\", validate_filenames = False,\n",
        "#                                            class_mode = None, target_size = (32, 32), batch_size = batch_size, \n",
        "#                                            shuffle = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.377138Z",
          "iopub.execute_input": "2022-01-08T10:34:09.37794Z",
          "iopub.status.idle": "2022-01-08T10:34:09.392295Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.377896Z",
          "shell.execute_reply": "2022-01-08T10:34:09.391513Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "YhrF3FhipJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_images *= 255\n",
        "testing_images *= 255\n",
        "#mixup, mixup_label = MixUp(training_images, training_images[::-1], labeling_images, labeling_images[::-1], alpha = 0.2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.394611Z",
          "iopub.execute_input": "2022-01-08T10:34:09.395178Z",
          "iopub.status.idle": "2022-01-08T10:34:09.505172Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.395143Z",
          "shell.execute_reply": "2022-01-08T10:34:09.50417Z"
        },
        "trusted": true,
        "id": "qDbgoxqwpJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets = 4\n",
        "labels_pred = np.zeros((test.shape[0], 10))\n",
        "\n",
        "labeling_images_temp = np.concatenate((labeling_images, labeling_images), axis = 0)\n",
        "    \n",
        "for i in range(nets):\n",
        "    training_images_temp = np.concatenate((training_images, np.array(Augmentation(training_images))), axis = 0)\n",
        "    try:\n",
        "        TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', TPU.master())\n",
        "    except ValueError:\n",
        "        print('Running on GPU or CPU')\n",
        "        TPU = None\n",
        "\n",
        "    if TPU:\n",
        "        tf.config.experimental_connect_to_cluster(TPU)\n",
        "        tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
        "    else:\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        policy = mixed_precision.Policy('mixed_float16')\n",
        "        mixed_precision.set_policy(policy)\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "    #==============================================\n",
        "    po_schedule = PolynomialDecay(maxEpochs = 150, initLR = 0.01, endLR = 0.001, power = 5.0)\n",
        "    step_schedule = StepDecay(initial_LR = 0.01, factor = 0.25, drop_every = 10)\n",
        "\n",
        "    opt_sgd = tf.keras.optimizers.SGD(learning_rate = 0.1 if TPU else 0.001, \n",
        "                                      momentum = 0.9) # momentum = 0.9, learning_rate = 0.1\n",
        "    opt_adam = tf.keras.optimizers.Adam(learning_rate = 0.01, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07)\n",
        "    opt_rmspop = tf.keras.optimizers.RMSprop(learning_rate = 0.01, rho = 0.9, momentum = 0.0, epsilon = 1e-07)\n",
        "    #==============================================\n",
        "    reduce_lr = ReduceLROnPlateau(factor = 0.9, patience = 5, min_lr = 1e-10, verbose = 1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience = 10, verbose = 1, mode = \"min\", \n",
        "                                   restore_best_weights = True)\n",
        "    lr_schedule = LearningRateScheduler(CustomDecay, verbose = 1)\n",
        "    checkpoint = ModelCheckpoint(filepath = f\"./CNN{i + 1}.h5\", save_weights_only = True, \n",
        "                                 monitor = \"val_accuracy\", mode = \"max\", save_best_only = True)\n",
        "    #==============================================\n",
        "    clear_output(wait = True)\n",
        "    print(f\"Current net: {i + 1}/{nets}\")\n",
        "\n",
        "    cnn = CNN6() # effb6\n",
        "    cnn.compile(optimizer = opt_sgd, loss = tf.keras.losses.CategoricalCrossentropy(from_logits = False), \n",
        "                   metrics = [\"accuracy\"], steps_per_execution = 2048 if TPU else 1)\n",
        "    #cnn.summary()\n",
        "\n",
        "    #cnn.fit(training_set, validation_data = validation_set, epochs = 100, steps_per_epoch = 1125, validation_steps = 125)\n",
        "    cnn.fit(training_images_temp, labeling_images_temp, validation_split = 0.1, epochs = 100 if TPU else 100, \n",
        "            callbacks = [reduce_lr], batch_size = 256 if TPU else 64)\n",
        "    #cnn.fit(traingen.flow(x_train, y_train, batch_size = batch_size, subset = \"training\"), \n",
        "    #        validation_data = traingen.flow(x_train, y_train, batch_size = 64, subset = \"validation\"), \n",
        "    #        epochs = 50, callbacks = [reduce_lr])\n",
        "    #cnn.fit(training_set_gen, validation_data = validation_set_gen, epochs = 50, callbacks = [reduce_lr])\n",
        "    #cnn.fit(x_train, y_train, validation_split = 0.1, epochs = 100 if TPU else 100, \n",
        "    #        batch_size = 1024 if TPU else 64, callbacks = [reduce_lr])\n",
        "\n",
        "    #cnn.load_weights(f\"./CNN{i + 1}.h5\")\n",
        "    labels_pred += cnn.predict(testing_images, verbose = 1)\n",
        "    #labels_pred += cnn.predict(test_set_gen, verbose = 1)\n",
        "    \n",
        "    cnn.save_weights(f\"./CNN{i + 1}.h5\")\n",
        "    #================================================\n",
        "    clear_output(wait = True)\n",
        "\n",
        "labels_pred = np.argmax(labels_pred, axis = 1)\n",
        "labels_pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T10:34:09.506638Z",
          "iopub.execute_input": "2022-01-08T10:34:09.506876Z",
          "iopub.status.idle": "2022-01-08T12:19:31.891037Z",
          "shell.execute_reply.started": "2022-01-08T10:34:09.506848Z",
          "shell.execute_reply": "2022-01-08T12:19:31.88998Z"
        },
        "trusted": true,
        "id": "4vs2fJ0dpJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_name = []\n",
        "for i in range(len(labels_pred)):\n",
        "    labels_name.append(labels[labels_pred[i]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T12:19:31.908803Z",
          "iopub.execute_input": "2022-01-08T12:19:31.909062Z",
          "iopub.status.idle": "2022-01-08T12:19:31.92581Z",
          "shell.execute_reply.started": "2022-01-08T12:19:31.909032Z",
          "shell.execute_reply": "2022-01-08T12:19:31.925048Z"
        },
        "trusted": true,
        "id": "kfGBKJrZpJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"../input/gdsc-ai-challenge/sampleSubmission.csv\").drop(columns = \"label\")\n",
        "submission[\"label\"] = labels_name\n",
        "submission.to_csv(\"./submission.csv\", index = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T12:19:31.927392Z",
          "iopub.execute_input": "2022-01-08T12:19:31.927655Z",
          "iopub.status.idle": "2022-01-08T12:19:32.00883Z",
          "shell.execute_reply.started": "2022-01-08T12:19:31.927626Z",
          "shell.execute_reply": "2022-01-08T12:19:32.007743Z"
        },
        "trusted": true,
        "id": "huceM0UGpJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T12:19:32.010146Z",
          "iopub.execute_input": "2022-01-08T12:19:32.010415Z",
          "iopub.status.idle": "2022-01-08T12:19:32.034902Z",
          "shell.execute_reply.started": "2022-01-08T12:19:32.010385Z",
          "shell.execute_reply": "2022-01-08T12:19:32.033846Z"
        },
        "trusted": true,
        "id": "WbnGAA5spJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn.save(\"./CNN.h5\")\n",
        "#cnn = tf.keras.models.load_model(\"./CNN.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-08T12:19:32.036473Z",
          "iopub.execute_input": "2022-01-08T12:19:32.036996Z",
          "iopub.status.idle": "2022-01-08T12:19:32.041303Z",
          "shell.execute_reply.started": "2022-01-08T12:19:32.036951Z",
          "shell.execute_reply": "2022-01-08T12:19:32.040308Z"
        },
        "trusted": true,
        "id": "lsIE2Wq9pJY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6qMsN8mypJY4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}